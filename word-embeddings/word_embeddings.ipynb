{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings can capture the \"context\" of a word. We can analyze word embeddings to understand the ideas associated with a word in a corpus.\n",
    "\n",
    "A well-trained set of word vectors provides insight into words that are closer to each other in meaning within a corpus. For example, if I queried the word \"Texas\" my model might tell me that \"California\" and \"Illinois\" are similar in meaning (becuase these are each states). If I instead queried the word \"red\" I might see that \"yellow\" and \"blue\" are similar in meaning (becuase these are each colors).\n",
    "\n",
    "We can leverage the insight gleened from a word embeddings model to see how language changed over time or across discourse communities. We can ask questions like: how does the U.S. Congress contextualize \"immigration\" in 2001 compared to 2021? Or, which words does the state school board associate with \"gay\" in Texas compared to New York? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(working_dir, fname):\n",
    "    # Read csv file as list of lists. \n",
    "    # Then clean the list of lists \n",
    "\n",
    "    with open(working_dir + fname, newline = '') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)[1:]\n",
    "            data = list(map(str, data))\n",
    "            \n",
    "    data = [re.sub(r'\\\\\\\\n|\\\\\\\\t|\\'s', '', word) for word in data] # remove line breaks, tab breaks, and possessive \"s\"\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', word) for word in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\d{1, 3}', '', word) for word in data] # remove digits that are a minimum of 1 and a maximum of 3\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', word) for word in data] # remove character strings that contain a digit\n",
    "        \n",
    "    data = [word.lower() for word in data]\n",
    "    data = [word.split() for word in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/home/stephbuon/data/'\n",
    "fname = 'congress_2001.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'majority', 'leader'],\n",
       " ['senator'],\n",
       " ['is', 'recognized'],\n",
       " ['mr', 'president'],\n",
       " ['on', 'behalf', 'of', 'the', 'entire', 'senate']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_import(working_dir, fname)\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now model our data. `Word2Vec()` uses a few unfamiliar words. Here, `workers` refers to the number of cores (aka \"brains\") in your laptop. This allows you to allocate work to more cores than just one. `min_count` tells our model not to consider words stated less than 20 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 1.15 s, total: 3min 12s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "period_model = gensim.models.Word2Vec(sentences = data, workers = 8, min_count = 20, vector_size = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with just one year of the U.S. Congressional Records. Modeling word embeddings can take a long time when working with large data sets, like the Congressional Records for 100 years. \n",
    "\n",
    "Ideally we would only create a work embeddings model from our data once, not every time we want to do an analysis. Lucky for us we can save our model to our computer for later.\n",
    "\n",
    "The following code creates a folder named `word_embeddings` in our working directory if the folder does not already exist and then saves our model to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = working_dir + 'word_embeddings'\n",
    "\n",
    "if not os.path.exists(working_folder):\n",
    "    os.mkdir(working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_model.save(working_dir + 'congress_2001_word_embeddings_model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our model whenever we want to work with our word embeddings some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_model = gensim.models.Word2Vec.load(working_dir + 'congress_2001_word_embeddings_model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embeddings model represents the words that are considered similar to one-another based on its training corpus. By exploring word embeddings, we gain insight into how members of Congress associated issues with one-another in the year 2001. \n",
    "\n",
    "Scores assigned to word embeddings range from 0 to 1. Larger scores are associated with greater similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which words are considered to be \"most similar\" to one-another within the U.S. Congressional Records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adults', 0.63141268491745),\n",
       " ('individuals', 0.6123476028442383),\n",
       " ('patients', 0.5769939422607422),\n",
       " ('families', 0.5762478709220886),\n",
       " ('nurses', 0.5738139748573303),\n",
       " ('immigrants', 0.573233425617218),\n",
       " ('americans', 0.569057047367096),\n",
       " ('soldiers', 0.5658395290374756),\n",
       " ('hispanics', 0.5551348328590393),\n",
       " ('africanamericans', 0.5544487237930298)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('women', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('servicemen', 0.7856674194335938),\n",
       " ('soldiers', 0.609350323677063),\n",
       " ('firemen', 0.6058236360549927),\n",
       " ('heroes', 0.5795339941978455),\n",
       " ('firefighters', 0.5795301795005798),\n",
       " ('patriots', 0.5669488906860352),\n",
       " ('countrymen', 0.5585413575172424),\n",
       " ('souls', 0.5551075339317322),\n",
       " ('rescuers', 0.5501298904418945),\n",
       " ('brave', 0.5463396906852722)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('men', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twin', 0.7170605659484863),\n",
       " ('camps', 0.6846996545791626),\n",
       " ('crashing', 0.6712399125099182),\n",
       " ('khobar', 0.6627557873725891),\n",
       " ('airliners', 0.6483810544013977),\n",
       " ('crashed', 0.6441882252693176),\n",
       " ('tents', 0.6228711009025574),\n",
       " ('flames', 0.6196696162223816),\n",
       " ('shadows', 0.6154047250747681),\n",
       " ('rubble', 0.6119235754013062)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('towers', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('journalist', 0.7668976783752441),\n",
       " ('man', 0.7554538249969482),\n",
       " ('woman', 0.7496949434280396),\n",
       " ('hero', 0.7309134006500244),\n",
       " ('statesman', 0.7268860936164856),\n",
       " ('warrior', 0.7024351358413696),\n",
       " ('aviator', 0.6905612349510193),\n",
       " ('firefighter', 0.6881330609321594),\n",
       " ('politician', 0.6843069791793823),\n",
       " ('veteran', 0.6815338134765625)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('soldier', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('navy', 0.8557624220848083),\n",
       " ('infantry', 0.759527325630188),\n",
       " ('commander', 0.7565478682518005),\n",
       " ('armys', 0.7555398344993591),\n",
       " ('marine', 0.7459832429885864),\n",
       " ('battalion', 0.739398717880249),\n",
       " ('airborne', 0.73427414894104),\n",
       " ('naval', 0.732434868812561),\n",
       " ('marines', 0.7063413262367249),\n",
       " ('wing', 0.6886097192764282)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('army', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('international', 0.610640823841095),\n",
       " ('climate', 0.6087414026260376),\n",
       " ('globalization', 0.5622814297676086),\n",
       " ('nearterm', 0.5595552325248718),\n",
       " ('systemic', 0.5546252727508545),\n",
       " ('warming', 0.545982837677002),\n",
       " ('multilateral', 0.5427759289741516),\n",
       " ('technological', 0.5351625084877014),\n",
       " ('dynamic', 0.5350406765937805),\n",
       " ('evolving', 0.5344930291175842)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('global', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can substract vectors to see which words are associated with one term, and not the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mental', 0.40712496638298035),\n",
       " ('medicaid', 0.39979588985443115),\n",
       " ('immunizations', 0.3840039372444153),\n",
       " ('referrals', 0.3836553394794464),\n",
       " ('outpatient', 0.37981879711151123),\n",
       " ('admissions', 0.37965041399002075),\n",
       " ('female', 0.37700706720352173),\n",
       " ('adolescent', 0.3649798333644867),\n",
       " ('exams', 0.3635528087615967),\n",
       " ('ag', 0.3635350167751312)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are associated with woman and not man? \n",
    "\n",
    "diff = period_model.wv['woman'] - period_model.wv['man']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.5116848349571228),\n",
       " ('honesty', 0.4083215296268463),\n",
       " ('bipartisanship', 0.397760808467865),\n",
       " ('gesture', 0.3768903613090515),\n",
       " ('affection', 0.3675096035003662),\n",
       " ('boundless', 0.36692705750465393),\n",
       " ('love', 0.36035898327827454),\n",
       " ('humility', 0.35714954137802124),\n",
       " ('exemplifies', 0.35624781250953674),\n",
       " ('warrior', 0.3549908399581909)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are associated with man and not woman? \n",
    "\n",
    "diff = period_model.wv['man'] - period_model.wv['woman']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the score that represents how \"similar\" any two words are within a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6093504"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.similarity('soldiers', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014649789"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.similarity('christian', 'rational')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6093504"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.similarity('soldiers', 'men')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(dir_path, fname):\n",
    "    # Read csv file as list of lists. \n",
    "    # Then clean the list of lists \n",
    "\n",
    "    with open(dir_path + fname, newline = '') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)[1:]\n",
    "            data = list(map(str, data))\n",
    "            \n",
    "    data = [re.sub(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b', '', ls) for ls in data] # remove words that are all upper case - so names \n",
    "    data = [re.sub(r'\\\\\\\\n|\\\\\\\\t|\\'s', '', ls) for ls in data] # remove line breaks, tab breaks, and possessive \"s\"\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', ls) for ls in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\d{1, 3}', '', ls) for ls in data] # remove digits that are a minimum of 1 and a maximum of 3\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', ls) for ls in data] # remove character strings that contain a digit\n",
    "        \n",
    "    data = [word.lower() for word in data]\n",
    "    data = [ls.split() for ls in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "data = data_import(dir_path, fname)\n",
    "                \n",
    "period_model = gensim.models.Word2Vec(sentences = data, workers = 8, min_count = 20, vector_size = 100) # remove words stated less than 20 times, size of neural net layers; default is 100 - go higher for larger corpora \n",
    "     \n",
    "period_model.save('congress_women_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "period_model = gensim.models.Word2Vec(sentences = data, workers = 8, min_count = 20, vector_size = 100) # remove words stated less than 20 times, size of neural net layers; default is 100 - go higher for larger corpora \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That sure takes awhile to run. Now imagine if we had an even larger data set! In an ideal situation, we would only run that code once -- not every time we want to analyze word embeddings. In luck\n",
    "\n",
    "We can save our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_model.save('congress_women_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model_1860 = gensim.models.Word2Vec.load(dir_path + 'hansard_1860_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

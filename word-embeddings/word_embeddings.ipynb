{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings can capture the the \"context\" of a word. \n",
    "\n",
    "A well-trained set of word vectors will seek to represent words that are closer to each other in meaning. For example: \"New York,\" \"California,\" and \"Texas,\" may be considered similar in meaning, while \"red,\" \"yellow,\" and \"blue\" may be considered similar in meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(dir_path, fname):\n",
    "    # Read csv file as list of lists. \n",
    "    # Then clean the list of lists \n",
    "\n",
    "    with open(dir_path + fname, newline = '') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)[1:]\n",
    "            data = list(map(str, data))\n",
    "            \n",
    "    data = [re.sub(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b', '', ls) for ls in data] # remove words that are all upper case - so names \n",
    "    data = [re.sub(r'\\\\\\\\n|\\\\\\\\t|\\'s', '', ls) for ls in data] # remove line breaks, tab breaks, and possessive \"s\"\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', ls) for ls in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\d{1, 3}', '', ls) for ls in data] # remove digits that are a minimum of 1 and a maximum of 3\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', ls) for ls in data] # remove character strings that contain a digit\n",
    "        \n",
    "    data = [word.lower() for word in data]\n",
    "    data = [ls.split() for ls in data]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/home/stephbuon/data/'\n",
    "fname = 'us_congress_2001.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'majority', 'leader'],\n",
       " ['senator'],\n",
       " ['is', 'recognized'],\n",
       " ['mr', 'president'],\n",
       " ['on', 'behalf', 'of', 'the', 'entire', 'senate']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_import(working_dir, fname)\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now model our data. `Word2Vec()` uses a few unfamiliar words. Here, `workers` refers to the number of cores (aka \"brains\") in your laptop. This allows you to allocate work to more cores than just one. `min_count` tells our model not to consider words stated less than 20 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 1.15 s, total: 3min 12s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "period_model = gensim.models.Word2Vec(sentences = data, workers = 8, min_count = 20, vector_size = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with just one year of the U.S. Congressional Records. Modeling word embeddings can take a long time when working with large data sets, like the Congressional Records for 100 years. \n",
    "\n",
    "Ideally we would only create a work embeddings model from our data once, not every time we want to do an analysis. Lucky for us we can save our model to our computer for later.\n",
    "\n",
    "The following code creates a folder named `word_embeddings` in our working directory if the folder does not already exist and then saves our model to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = working_dir + 'word_embeddings'\n",
    "\n",
    "if not os.path.exists(working_folder):\n",
    "    os.mkdir(working_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_model.save(working_dir + 'congress_2001_word_embeddings_model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our model whenever we want to work with our word embeddings some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_model = gensim.models.Word2Vec.load(working_dir + 'congress_2001_word_embeddings_model.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Our Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embeddings model represents the words that are considered similar to one-another based on its training corpus. By exploring word embeddings, we gain insight into how members of Congress associated issues with one-another in the year 2001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.791344404220581),\n",
       " ('person', 0.7732507586479187),\n",
       " ('soldier', 0.7496948838233948),\n",
       " ('citizen', 0.6800270080566406),\n",
       " ('politician', 0.6747932434082031),\n",
       " ('mother', 0.6740400791168213),\n",
       " ('lady', 0.640896737575531),\n",
       " ('girl', 0.6405008435249329),\n",
       " ('teenager', 0.6390302181243896),\n",
       " ('lawyer', 0.63542640209198)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('woman', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twin', 0.7170605659484863),\n",
       " ('camps', 0.6846996545791626),\n",
       " ('crashing', 0.6712399125099182),\n",
       " ('khobar', 0.6627557873725891),\n",
       " ('airliners', 0.6483810544013977),\n",
       " ('crashed', 0.6441882252693176),\n",
       " ('tents', 0.6228711009025574),\n",
       " ('flames', 0.6196696162223816),\n",
       " ('shadows', 0.6154047250747681),\n",
       " ('rubble', 0.6119235754013062)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('towers', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('journalist', 0.7668976783752441),\n",
       " ('man', 0.7554538249969482),\n",
       " ('woman', 0.7496949434280396),\n",
       " ('hero', 0.7309134006500244),\n",
       " ('statesman', 0.7268860936164856),\n",
       " ('warrior', 0.7024351358413696),\n",
       " ('aviator', 0.6905612349510193),\n",
       " ('firefighter', 0.6881330609321594),\n",
       " ('politician', 0.6843069791793823),\n",
       " ('veteran', 0.6815338134765625)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('soldier', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('navy', 0.8557624220848083),\n",
       " ('infantry', 0.759527325630188),\n",
       " ('commander', 0.7565478682518005),\n",
       " ('armys', 0.7555398344993591),\n",
       " ('marine', 0.7459832429885864),\n",
       " ('battalion', 0.739398717880249),\n",
       " ('airborne', 0.73427414894104),\n",
       " ('naval', 0.732434868812561),\n",
       " ('marines', 0.7063413262367249),\n",
       " ('wing', 0.6886097192764282)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_model.wv.most_similar('army', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can substract vectors to see which words are associated with one term, and not the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('army', 0.6420791149139404),\n",
       " ('infantryman', 0.4506945013999939),\n",
       " ('philippine', 0.4179878532886505),\n",
       " ('liberation', 0.3949863314628601),\n",
       " ('commander', 0.3947765827178955),\n",
       " ('infantry', 0.3868419826030731),\n",
       " ('invasion', 0.37694051861763),\n",
       " ('allied', 0.3742254078388214),\n",
       " ('marine', 0.37334108352661133),\n",
       " ('filipino', 0.3707565367221832)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are associated with ENTER and not ENTER? \n",
    "\n",
    "diff = period_model.wv['army'] - period_model.wv['navy']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mental', 0.40712496638298035),\n",
       " ('medicaid', 0.39979588985443115),\n",
       " ('immunizations', 0.3840039372444153),\n",
       " ('referrals', 0.3836553394794464),\n",
       " ('outpatient', 0.37981879711151123),\n",
       " ('admissions', 0.37965041399002075),\n",
       " ('female', 0.37700706720352173),\n",
       " ('adolescent', 0.3649798333644867),\n",
       " ('exams', 0.3635528087615967),\n",
       " ('ag', 0.3635350167751312)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are associated with woman and not man? \n",
    "\n",
    "diff = period_model.wv['woman'] - period_model.wv['man']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.5116848349571228),\n",
       " ('honesty', 0.4083215296268463),\n",
       " ('bipartisanship', 0.397760808467865),\n",
       " ('gesture', 0.3768903613090515),\n",
       " ('affection', 0.3675096035003662),\n",
       " ('boundless', 0.36692705750465393),\n",
       " ('love', 0.36035898327827454),\n",
       " ('humility', 0.35714954137802124),\n",
       " ('exemplifies', 0.35624781250953674),\n",
       " ('warrior', 0.3549908399581909)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are associated with man and not woman? \n",
    "\n",
    "diff = period_model.wv['man'] - period_model.wv['woman']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universitys', 0.62779700756073),\n",
       " ('pioneering', 0.5837754607200623),\n",
       " ('brown', 0.5735412836074829),\n",
       " ('marshall', 0.5511050224304199),\n",
       " ('christian', 0.5446071028709412),\n",
       " ('cal', 0.5439032316207886),\n",
       " ('ripken', 0.5431542992591858),\n",
       " ('episcopal', 0.536072850227356),\n",
       " ('johns', 0.5347297191619873),\n",
       " ('william', 0.5316272377967834)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are similar to man and not woman? \n",
    "\n",
    "diff = period_model.wv['christian'] - period_model.wv['muslim']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exports', 0.4480922520160675),\n",
       " ('arab', 0.42241916060447693),\n",
       " ('imports', 0.4129955768585205),\n",
       " ('muslims', 0.4064711928367615),\n",
       " ('wool', 0.4007987678050995),\n",
       " ('except', 0.3967370390892029),\n",
       " ('estates', 0.3811449110507965),\n",
       " ('ome', 0.37807828187942505),\n",
       " ('somehow', 0.37490472197532654),\n",
       " ('are', 0.3723890483379364)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are similar to man and not woman? \n",
    "\n",
    "diff = period_model.wv['muslim'] - period_model.wv['christian']\n",
    "period_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model_1860.wv.similarity('soldiers', 'men')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

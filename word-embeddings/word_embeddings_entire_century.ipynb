{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings can capture the the \"context\" of a word. A well-trained set of word vectors will place similar words close to each other in meaning. For example: \"New York,\" \"California,\" and \"Texas,\" will cluster in one corner, while \"red,\" \"yellow,\" and \"blue\" cluster together in another corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/stephbuon/data/hansard_decades/'\n",
    "n_cores = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embedding Models by Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 10.5 µs\n"
     ]
    }
   ],
   "source": [
    "def data_import(dir_path, fname):\n",
    "    # Read csv file as list of lists. \n",
    "    # Then clean the list of lists \n",
    "\n",
    "    with open(dir_path + fname, newline = '') as f:\n",
    "            reader = csv.reader(f)\n",
    "            data = list(reader)[1:]\n",
    "            data = list(map(str, data))\n",
    "            \n",
    "    data = [re.sub(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b', '', ls) for ls in data] # remove words that are all upper case - so names \n",
    "    data = [re.sub(r'\\\\\\\\n|\\\\\\\\t|\\'s', '', ls) for ls in data] # remove line breaks, tab breaks, and possessive \"s\"\n",
    "    data = [re.sub(r'[^\\w\\s]|_', '', ls) for ls in data] # remove punctuation and underscore\n",
    "    data = [re.sub(r'\\d{1, 3}', '', ls) for ls in data] # remove digits that are a minimum of 1 and a maximum of 3\n",
    "    data = [re.sub(r'\\w*\\d\\w*', '', ls) for ls in data] # remove character strings that contain a digit\n",
    "        \n",
    "    data = [word.lower() for word in data]\n",
    "    data = [ls.split() for ls in data]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def export_model_by_period(dir_path):\n",
    "    # create, name, and export word embedding models for each time period. \n",
    "\n",
    "    cycle = 0\n",
    "    for fname in os.listdir(dir_path):\n",
    "        if '.csv' in fname:\n",
    "            cycle = cycle + 1\n",
    "\n",
    "            data = data_import(dir_path, fname)\n",
    "                \n",
    "            period_model = gensim.models.Word2Vec(sentences = data,\n",
    "                                              workers = n_cores, \n",
    "                                              min_count = 20, # remove words stated less than 20 times\n",
    "                                              vector_size = 100) # size of neural net layers; default is 100 - go higher for larger corpora \n",
    "                \n",
    "            extention_position = fname.index('.')\n",
    "            fname = fname[0:extention_position]\n",
    "                \n",
    "            if cycle == 1:\n",
    "                congress_model = period_model\n",
    "            else:\n",
    "                congress_model.build_vocab(data, update = True)\n",
    "                congress_model.train(data, total_examples = period_model.corpus_count, epochs = period_model.epochs)\n",
    "        \n",
    "            save_name = os.path.join(dir_path, fname)\n",
    "            congress_model.save(save_name + '_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "export_model_by_period(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a decade of the corpus to see which words are \"most similar\" to \"coal,\" \"crime,\" \"disease,\" \"man,\" or \"woman.\" Feel free to load a different decade of the corpus or to search your own word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model_1860 = gensim.models.Word2Vec.load(dir_path + 'hansard_1860_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('timber', 0.792468249797821),\n",
       " ('coals', 0.783724844455719),\n",
       " ('cotton', 0.7810822129249573),\n",
       " ('wool', 0.7700701951980591),\n",
       " ('ore', 0.7629643082618713),\n",
       " ('yarn', 0.7509469985961914),\n",
       " ('salt', 0.7386558651924133),\n",
       " ('copper', 0.7379512190818787),\n",
       " ('flour', 0.7337479591369629),\n",
       " ('grain', 0.7298588752746582)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('coal', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drunkenness', 0.7556180357933044),\n",
       " ('crimes', 0.7345277070999146),\n",
       " ('boycotting', 0.7077376842498779),\n",
       " ('insubordination', 0.694895327091217),\n",
       " ('outrages', 0.6794412136077881),\n",
       " ('outrage', 0.6774588823318481),\n",
       " ('conspiracy', 0.675053596496582),\n",
       " ('murder', 0.671721875667572),\n",
       " ('immorality', 0.6593766808509827),\n",
       " ('offences', 0.6577752828598022)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('crime', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('infection', 0.7841604351997375),\n",
       " ('cholera', 0.7486963272094727),\n",
       " ('pleuropneumonia', 0.7468662261962891),\n",
       " ('smallpox', 0.7328136563301086),\n",
       " ('plague', 0.7072739601135254),\n",
       " ('fever', 0.6894288659095764),\n",
       " ('sickness', 0.682775616645813),\n",
       " ('diseases', 0.6741213202476501),\n",
       " ('epidemic', 0.6731758117675781),\n",
       " ('contagion', 0.66523277759552)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('disease', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dublin', 0.8127764463424683),\n",
       " ('glasgow', 0.8059346079826355),\n",
       " ('edinburgh', 0.7964843511581421),\n",
       " ('belfast', 0.713414192199707),\n",
       " ('manchester', 0.7014787197113037),\n",
       " ('liverpool', 0.7004226446151733),\n",
       " ('loudon', 0.6925938129425049),\n",
       " ('bristol', 0.691468358039856),\n",
       " ('leeds', 0.6817807555198669),\n",
       " ('birmingham', 0.6801958084106445)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('london', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 0.8021567463874817),\n",
       " ('boy', 0.7457062005996704),\n",
       " ('woman', 0.7360450625419617),\n",
       " ('policeman', 0.7186694145202637),\n",
       " ('lawyer', 0.6999873518943787),\n",
       " ('workman', 0.6976069808006287),\n",
       " ('nobleman', 0.6952993273735046),\n",
       " ('soldier', 0.6944471597671509),\n",
       " ('fool', 0.6806564927101135),\n",
       " ('lad', 0.6732933521270752)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('man', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.8324806690216064),\n",
       " ('boy', 0.7969318628311157),\n",
       " ('child', 0.7723746299743652),\n",
       " ('husband', 0.7638962268829346),\n",
       " ('policeman', 0.7538986802101135),\n",
       " ('daughter', 0.7533218860626221),\n",
       " ('widow', 0.7492982149124146),\n",
       " ('lady', 0.7480725049972534),\n",
       " ('man', 0.7360450625419617),\n",
       " ('wife', 0.7273796200752258)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('woman', topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sugar', 0.8062563538551331),\n",
       " ('wheat', 0.7994242906570435),\n",
       " ('barley', 0.7566043734550476),\n",
       " ('grain', 0.7208844423294067),\n",
       " ('wool', 0.7201630473136902),\n",
       " ('malt', 0.7100582122802734),\n",
       " ('flour', 0.7097944617271423),\n",
       " ('hops', 0.6989511251449585),\n",
       " ('tobacco', 0.6885563731193542),\n",
       " ('maize', 0.6872325539588928)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.most_similar('corn', topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting Vectors\n",
    "(e.g.: \"first word\" - \"second word\" = ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('illegitimate', 0.3977678418159485),\n",
       " ('underfed', 0.3500392735004425),\n",
       " ('marriages', 0.34574127197265625),\n",
       " ('vaccine', 0.3394405245780945),\n",
       " ('grandchildren', 0.33773303031921387),\n",
       " ('dying', 0.33594730496406555),\n",
       " ('sickness', 0.3323615789413452),\n",
       " ('infant', 0.3310065269470215),\n",
       " ('deportation', 0.3309376537799835),\n",
       " ('zones', 0.3264090120792389)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are similar to woman and not man? \n",
    "\n",
    "diff = congress_model_1860.wv['woman'] - congress_model_1860.wv['man']\n",
    "congress_model_1860.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.6192151308059692),\n",
       " ('statesman', 0.48818546533584595),\n",
       " ('lawyer', 0.4785446226596832),\n",
       " ('critic', 0.43419334292411804),\n",
       " ('person', 0.42894574999809265),\n",
       " ('nobleman', 0.4061669707298279),\n",
       " ('civilian', 0.40244725346565247),\n",
       " ('member', 0.4014107286930084),\n",
       " ('anybody', 0.39986884593963623),\n",
       " ('anyone', 0.38719943165779114)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are similar to man and not woman? \n",
    "\n",
    "diff = congress_model_1860.wv['man'] - congress_model_1860.wv['woman']\n",
    "congress_model_1860.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similarity Score\n",
    "(e.g.: how similar is \"first word\" to \"second word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7297269"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.similarity('soldiers', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5561376"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.similarity('women', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59991306"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model_1860.wv.similarity('prostitute', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Word Embeddings By Decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from numpy import linspace\n",
    "from adjustText import adjust_text\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_embeddings:\n",
    "    \n",
    "    def keyword_context_find_difference(dir_path, keyword_, keyword2):\n",
    "\n",
    "        keyword_context = []\n",
    "        for fname in os.listdir(dir_path):\n",
    "            if '_model' in fname:\n",
    "                congress_model = gensim.models.Word2Vec.load(dir_path + fname)\n",
    "                diff = congress_model.wv[keyword_] - congress_model.wv[keyword2] # these two are one word but not the other. \n",
    "                keyword_context_period = congress_model.wv.similar_by_vector(diff, topn = 100)\n",
    "                keyword_context.append(keyword_context_period)\n",
    "            \n",
    "            else:\n",
    "                keyword_context.append([]) \n",
    "                \n",
    "        return keyword_context\n",
    "\n",
    "    def keyword_context_find_most_similar(dir_path, keyword_):\n",
    "        \n",
    "        keyword_context = []\n",
    "        for fname in os.listdir(dir_path):\n",
    "            if '_model' in fname:\n",
    "                congress_model = gensim.models.Word2Vec.load(dir_path + fname)\n",
    "                keyword_context_period = congress_model.wv.most_similar(keyword_, topn = 100) # most similar \n",
    "                keyword_context.append(keyword_context_period)\n",
    "\n",
    "            else:\n",
    "                keyword_context.append([]) \n",
    "                \n",
    "        return keyword_context\n",
    "\n",
    "    \n",
    "    \n",
    "class w2v_visualize_scatter_plot:\n",
    "    \n",
    "    def label_periods(start, end, interval):\n",
    "        periods = range(start, end, interval)\n",
    "        period_names = [str(period) + '.0' for period in periods]\n",
    "        return period_names\n",
    "\n",
    "\n",
    "    def collect_text_values(keyword_context, period_names):\n",
    "        period_words = []\n",
    "    \n",
    "        for i in range(0, len(period_names)):\n",
    "            try:\n",
    "                words = [value[0] for value in keyword_context[i]]\n",
    "                period_words.append(words)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        return period_words\n",
    "        \n",
    "        \n",
    "    def make_1D_list(period_words):\n",
    "        flat_list = []\n",
    "    \n",
    "        for list in period_words:\n",
    "            for word in list:\n",
    "                flat_list.append(word)\n",
    "                \n",
    "        return flat_list\n",
    "    \n",
    "    \n",
    "    def w2v_scatter_plot(period_names, keyword_context, flat_list, keyword): # add a kw argument for title -- like Hansard debates \n",
    "        colors = [ cm.gnuplot(x) for x in linspace(0, 1, len(flat_list)) ]\n",
    "    \n",
    "        plt.figure(figsize=(30, 30), dpi = 1000)\n",
    "\n",
    "        texts = []\n",
    "\n",
    "        # plt.annotate only plots one label per iteration, so we have to use a for loop \n",
    "        for i in range(0,len(period_names)): # cycle through the period names                     \n",
    "            for j in range(15): # cycle through the first ten words (you can change this variable)\n",
    "                if keyword_context[i]:\n",
    "                    xx = period_names[i] # on the x axis, plot the period name\n",
    "                    yy = [item[1] for item in keyword_context[i]][j] # on the y axis, plot the distance -- how closely the word is related to the keyword\n",
    "                    txt = [item[0] for item in keyword_context[i]][j] # grab the name of each collocated word\n",
    "                    colorindex = flat_list.index(txt) # this command keeps all dots for the same word the same color\n",
    "        \n",
    "                    plt.scatter(  # plot dots\n",
    "                        xx, # x axis\n",
    "                        yy, # y axis\n",
    "                        linewidth=1, \n",
    "                        color = colors[colorindex],\n",
    "                        s = 300, # dot size\n",
    "                        alpha=0.5) # dot transparency\n",
    "\n",
    "                    texts.append(plt.text(xx, yy, txt)) # make a label for each word\n",
    "\n",
    "        adjust_text(texts, force_points=0.0001, force_text=0.0035, # Code to help with overlapping labels -- may take a minute to run\n",
    "                            expand_points=(2, 2), expand_text=(2, 2), # from 1, 1 \n",
    "                            arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(\"What words were most associated with ''\" + keyword + \"' in the Hansard debates?\", fontsize=20, fontweight=0, color='Red')\n",
    "        plt.xlabel(\"period\")\n",
    "        plt.ylabel(\"similarity to \" + keyword)\n",
    "        plt.savefig(keyword + '_' + period_names[1] + '_' + period_names[-1] +'.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "keyword_context = w2v_embeddings.keyword_context_find_difference(dir_path, 'woman', 'man')\n",
    "period_names = w2v_visualize_scatter_plot.label_periods(1800, 1900, 5)\n",
    "period_words = w2v_visualize_scatter_plot.collect_text_values(keyword_context, period_names)\n",
    "flat_list = w2v_visualize_scatter_plot.make_1D_list(period_words)    \n",
    "\n",
    "w2v_visualize_scatter_plot.w2v_scatter_plot(period_names, keyword_context, flat_list, 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "keywords_list = ['education', 'coal', 'railway', 'corn']\n",
    "\n",
    "for keyword in keywords_list:\n",
    "    keyword_context = w2v_embeddings.keyword_context_find_most_similar(dir_path, keyword)\n",
    "    period_names = w2v_visualize_scatter_plot.label_periods(1800, 1900, 5)\n",
    "    period_words = w2v_visualize_scatter_plot.collect_text_values(keyword_context, period_names)\n",
    "    flat_list = w2v_visualize_scatter_plot.make_1D_list(period_words)    \n",
    "    \n",
    "    try:\n",
    "        w2v_visualize_scatter_plot.w2v_scatter_plot(period_names, keyword_context, flat_list, keyword)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_vector = congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(woman_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'iraq' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-ee3508954a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iraq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'iraq' not present\""
     ]
    }
   ],
   "source": [
    "congress_model.wv.most_similar(\"iraq\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"america\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"britain\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['man'] - congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['woman'] - congress_model.wv['boy']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'the' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-489fee53d02a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeyword_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcongress_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcongress_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeyword_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyword_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'the' not present\""
     ]
    }
   ],
   "source": [
    "keyword_context = [word[0] for word in congress_model.wv.most_similar(\"the\", topn = 100)]\n",
    "\n",
    "sum = congress_model.wv[keyword_context[0]] \n",
    "\n",
    "for word in keyword_context[1:len(keyword_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "    \n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('soldier', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodnames = all_data['5yrperiod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "\n",
    "    # grab the data from period1\n",
    "    period_data = sample_m[sample_m['5yrperiod'] == period1] # select one period at a time\n",
    "    \n",
    "    # structure the data for Gensim\n",
    "    period_sentences = structure_data(period_data['speech'], lemma = False, stopwords = True, stemmed = True)\n",
    "    \n",
    "    # make the Gensim model\n",
    "    period_model = gensim.models.Word2Vec( # make a gensim model for that data\n",
    "        sentences = period_sentences,\n",
    "        min_count = 2, \n",
    "        size = 100)\n",
    "    \n",
    "    # save it\n",
    "    period_model.save(dataname + '-model-' + str(period1)) # save the model with the name of the period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword1 = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  after the first run, use this line to call the old data without generating it again\n",
    "keyword_context = []\n",
    "dates_found = []\n",
    "\n",
    "# cycle through each period\n",
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "    \n",
    "    # load the model from period1\n",
    "    period_model = gensim.models.Word2Vec.load(dataname + '-model-' + str(period1)) # to load a saved model\n",
    "\n",
    "    ## is the keyword found?\n",
    "    if keyword1 in period_model.wv.key_to_index:\n",
    "        print('found ', keyword1)\n",
    "        \n",
    "        # get the context vector for keyword1\n",
    "        keyword_context_period = period_model.wv.most_similar(keyword1, topn = 5000) \n",
    "        \n",
    "        # save it for later\n",
    "        keyword_context.append(keyword_context_period) # save the context of how women were talked about for later\n",
    "        dates_found.append(period1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to abstract only unique values while keeping the list in the same order -- the order of first appearance\n",
    "def unique2(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(len(dates_found)):\n",
    "    words = [item[0] for item in keyword_context[i]][:10]\n",
    "    all_words.append(words)\n",
    "\n",
    "all_words2 = []\n",
    "for list in all_words:\n",
    "    for word in list:\n",
    "        all_words2.append(word)\n",
    "\n",
    "numwords = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb Cell 43'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb#ch0000042?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m linspace\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb#ch0000042?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m cm\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb#ch0000042?line=7'>8</a>\u001b[0m colors \u001b[39m=\u001b[39m [ cm\u001b[39m.\u001b[39mviridis(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m linspace(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(unique2(all_words2))\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m) ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb#ch0000042?line=9'>10</a>\u001b[0m \u001b[39m# change the figure's size here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephbuon/projects/faha/word-embeddings/santafe_wordembeddings.ipynb#ch0000042?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m), dpi \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique2' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from numpy import linspace\n",
    "from matplotlib import cm\n",
    "\n",
    "colors = [ cm.viridis(x) for x in linspace(0, 1, len(unique2(all_words2))+10) ]\n",
    "\n",
    "# change the figure's size here\n",
    "plt.figure(figsize=(10,10), dpi = 200)\n",
    "\n",
    "texts = []\n",
    "\n",
    "# plt.annotate only plots one label per iteration, so we have to use a for loop \n",
    "for i in range(len(dates_found)):    # cycle through the period names\n",
    "    \n",
    "    #yyy = int(keyword_per_year[keyword_per_year['5yrperiod'] == int(xx)]['count'])   # how many times was the keyword used that year?\n",
    "                     \n",
    "    for j in range(10):     # cycle through the first ten words (you can change this variable)\n",
    "        \n",
    "        xx = dates_found[i]        # on the x axis, plot the period name\n",
    "        yy = [item[1] for item in keyword_context[i]][j]         # on the y axis, plot the distance -- how closely the word is related to the keyword\n",
    "        txt = [item[0] for item in keyword_context[i]][j]        # grab the name of each collocated word\n",
    "        colorindex = unique2(all_words2).index(txt)   # this command keeps all dots for the same word the same color\n",
    "        \n",
    "        plt.scatter(                                             # plot dots\n",
    "            xx, #x axis\n",
    "            yy, # y axis\n",
    "            linewidth=1, \n",
    "            color = colors[colorindex],\n",
    "            edgecolors = 'darkgray',\n",
    "            s = 100, # dot size\n",
    "            alpha=0.8)  # dot transparency\n",
    "\n",
    "        # make a label for each word\n",
    "        texts.append(plt.text(xx, yy, txt))\n",
    "\n",
    "# Code to help with overlapping labels -- may take a minute to run\n",
    "adjust_text(texts, force_points=0.2, force_text=.7, \n",
    "                    expand_points=(1, 1), expand_text=(1, 1),\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"What words were most similar to ''\" + keyword1 + \"' in Congress?\", fontsize=20, fontweight=0, color='Red')\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"similarity to \" + keyword1)\n",
    "\n",
    "\n",
    "filename = 'words-similar-to-' + keyword1 + '-' + dataname\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "This Notebook was developed by Steph Buongiorno. Code to visualize the word embeddings was taken from Jo Guldi's course on Digital History (see: https://github.com/stephbuon/digital-history/tree/master/hist3368-week12-word-context-vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

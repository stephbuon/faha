{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Gendered Constructions\n",
    "\n",
    "Look at Laura Klein NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, warnings; warnings.simplefilter('ignore')\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/scratch/group/pract-txt-mine/sbuongiorno/hansard_decades/hansard_1800.csv'\n",
    "\n",
    "with open(fname, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)[1:]\n",
    "    data = list(map(str, data))\n",
    "\n",
    "data = data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [re.sub(r'\\b[A-Z]+(?:\\s+[A-Z]+)*\\b', '', ls) for ls in data] # remove words that are all upper case - so names \n",
    "data = [re.sub(r'\\\\\\\\n|\\\\\\\\t|\\'s', '', ls) for ls in data] # remove lien breaks, tab breaks, and possessive \"s\"\n",
    "data = [re.sub(r'[^\\w\\s]|_', '', ls) for ls in data] # remove punctuation and underscore\n",
    "data = [re.sub(r'\\d{1, 3}', '', ls) for ls in data] # remove digits that are a minimum of 1 and a maximum of 3\n",
    "data = [re.sub(r'\\w*\\d\\w*', '', ls) for ls in data] # remove character strings that contain a digit\n",
    "            \n",
    "data = [word.lower() for word in data]\n",
    "\n",
    "data = ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTokens(spacy_doc_object):\n",
    "    spacy_tokens = []\n",
    "    for token in spacy_doc_object:\n",
    "        row = (token.text, token.lemma_, token.pos_, token.dep_, token.head.text)\n",
    "        spacy_tokens.append(row)\n",
    "    return spacy_tokens\n",
    "\n",
    "reddit_tokens = extractTokens(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', ' ', 'SPACE', '', 'moved'),\n",
       " ('moved', 'move', 'VERB', 'ROOT', 'moved'),\n",
       " ('that', 'that', 'SCONJ', 'mark', 'appointed'),\n",
       " ('lord', 'lord', 'PROPN', 'compound', 'walsingham'),\n",
       " ('walsingham', 'walsingham', 'PROPN', 'nsubjpass', 'appointed'),\n",
       " ('be', 'be', 'AUX', 'auxpass', 'appointed'),\n",
       " ('appointed', 'appoint', 'VERB', 'ccomp', 'moved'),\n",
       " ('chairman', 'chairman', 'NOUN', 'oprd', 'appointed'),\n",
       " ('of', 'of', 'ADP', 'prep', 'chairman'),\n",
       " ('the', 'the', 'DET', 'det', 'committee')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "move\n",
      "that\n",
      "lord\n",
      "walsingham\n",
      "be\n",
      "appoint\n",
      "chairman\n",
      "of\n",
      "the\n",
      "committee\n",
      "of\n",
      "privilege\n",
      "for\n",
      "the\n",
      "present\n",
      "session\n",
      " \n",
      "the\n",
      "subject\n",
      "in\n",
      "-PRON-\n",
      "contemplation\n",
      "be\n",
      "such\n",
      "as\n",
      "not\n",
      "only\n",
      "that\n",
      "house\n",
      "but\n",
      "many\n",
      "thousand\n",
      "of\n",
      "-PRON-\n",
      "majesty\n",
      "subject\n",
      "must\n",
      "regard\n",
      "with\n",
      "the\n",
      "utmost\n",
      "concern\n",
      " \n",
      "-PRON-\n",
      "agree\n",
      "with\n",
      "the\n",
      "noble\n",
      "earl\n",
      "as\n",
      "to\n",
      "the\n",
      "great\n",
      "importance\n",
      "of\n",
      "the\n",
      "subject\n",
      "-PRON-\n",
      "be\n",
      "one\n",
      "therefore\n",
      "which\n",
      "naturally\n",
      "attract\n",
      "the\n",
      "serious\n",
      "attention\n",
      "of\n",
      "-PRON-\n",
      "majesty\n",
      "government\n",
      "-PRON-\n",
      "lordship\n",
      "however\n",
      "would\n",
      "be\n",
      "at\n",
      "the\n",
      "same\n",
      "time\n",
      "aware\n",
      "of\n",
      "the\n",
      "complexity\n",
      "and\n",
      "intricate\n",
      "nature\n",
      "of\n",
      "the\n",
      "general\n",
      "subject\n",
      "the\n",
      "variety\n",
      "of\n",
      "detail\n",
      "which\n",
      "-PRON-\n",
      "embrace\n",
      "and\n",
      "the\n",
      "correspondent\n",
      "difficulty\n",
      "of\n",
      "form\n",
      "adequate\n",
      "regulation\n",
      " \n",
      "-PRON-\n",
      "be\n",
      "convince\n",
      "for\n",
      "-PRON-\n",
      "own\n",
      "part\n",
      "that\n",
      "the\n",
      "contrary\n",
      "be\n",
      "the\n",
      "fact\n",
      "and\n",
      "though\n",
      "die\n",
      "abuse\n",
      "have\n",
      "not\n",
      "be\n",
      "flagrant\n",
      "or\n",
      "excessive\n",
      "that\n",
      "the\n",
      "bank\n",
      "have\n",
      "yield\n",
      "in\n",
      "a\n",
      "certain\n",
      "degree\n",
      "to\n",
      "the\n",
      "temptation\n",
      "and\n",
      "extend\n",
      "the\n",
      "quantity\n",
      "of\n",
      "-PRON-\n",
      "note\n",
      "beyond\n",
      "the\n",
      "proper\n",
      "limit\n",
      " \n",
      "at\n",
      "present\n",
      "-PRON-\n",
      "be\n",
      "  \n",
      "the\n",
      "period\n",
      "of\n",
      "-PRON-\n",
      "present\n",
      "bill\n",
      "be\n",
      "somewhat\n",
      "different\n",
      "from\n",
      "that\n",
      "of\n",
      "the\n",
      "above\n",
      "year\n",
      "and\n",
      "be\n",
      "in\n",
      "-PRON-\n",
      "nature\n",
      "more\n",
      "definite\n",
      " \n",
      "-PRON-\n",
      "be\n",
      "one\n",
      "of\n",
      "those\n",
      "general\n",
      "remark\n",
      "which\n",
      "apply\n",
      "equally\n",
      "to\n",
      "every\n",
      "new\n",
      "proposition\n",
      "and\n",
      "be\n",
      "therefore\n",
      "undeserve\n",
      "of\n",
      "attention\n",
      "upon\n",
      "any\n",
      " \n",
      "lord\n",
      "hawkesbury\n",
      "what\n",
      "be\n",
      "the\n",
      "amount\n",
      "of\n",
      "this\n",
      "sum\n",
      "what\n",
      "be\n",
      "-PRON-\n",
      "intend\n",
      "application\n",
      "and\n",
      "whether\n",
      "-PRON-\n",
      "be\n",
      "destine\n",
      "to\n",
      "go\n",
      "to\n",
      "the\n",
      "consolidated\n",
      "fund\n",
      " \n",
      "-PRON-\n",
      "feel\n",
      "-PRON-\n",
      "authorize\n",
      "in\n",
      "pursue\n",
      "this\n",
      "course\n",
      "more\n",
      "particularly\n",
      "when\n",
      "-PRON-\n",
      "consider\n",
      "that\n",
      "the\n",
      "same\n",
      "argument\n",
      "which\n",
      "would\n",
      "convince\n",
      "-PRON-\n",
      "lordship\n",
      "of\n",
      "the\n",
      "necessity\n",
      "and\n",
      "expediency\n",
      "of\n",
      "the\n",
      "one\n",
      "bill\n",
      "would\n",
      "be\n",
      "equally\n",
      "applicable\n",
      "to\n",
      "the\n",
      "reenactment\n",
      "of\n",
      "the\n",
      "other\n",
      " \n",
      "the\n",
      "result\n",
      "of\n",
      "this\n",
      "be\n",
      "not\n",
      "unknown\n",
      "to\n",
      "-PRON-\n",
      "lordship\n",
      "and\n",
      "-PRON-\n",
      "be\n",
      "not\n",
      "unfair\n",
      "from\n",
      "this\n",
      "past\n",
      "experience\n",
      "to\n",
      "infer\n",
      "that\n",
      "the\n",
      "same\n",
      "individual\n",
      "who\n",
      "have\n",
      "exercise\n",
      "-PRON-\n",
      "power\n",
      "with\n",
      "such\n",
      "mode\n",
      "ration\n",
      "would\n",
      "not\n",
      "be\n",
      "likely\n",
      "to\n",
      "abuse\n",
      "-PRON-\n",
      "in\n",
      "future\n",
      " \n",
      "-PRON-\n",
      "be\n",
      "the\n",
      "result\n",
      "of\n",
      "a\n",
      "conviction\n",
      "on\n",
      "the\n",
      "part\n",
      "of\n",
      "minister\n",
      "that\n",
      "the\n",
      "loyalty\n",
      "of\n",
      "the\n",
      "great\n",
      "mass\n",
      "of\n",
      "the\n",
      "population\n",
      "be\n",
      "sound\n",
      "to\n",
      "the\n",
      "core\n",
      " \n",
      "this\n",
      "be\n",
      "the\n",
      "conclusion\n",
      "which\n",
      "a\n",
      "general\n",
      "knowledge\n",
      "of\n",
      "human\n",
      "nature\n",
      "justified\n",
      "and\n",
      "certainly\n",
      "event\n",
      "have\n",
      "fully\n",
      "demonstrate\n",
      "-PRON-\n",
      "truth\n",
      " \n",
      "-PRON-\n",
      "lordship\n",
      "would\n",
      "recollect\n",
      "that\n",
      "-PRON-\n",
      "majesty\n",
      "by\n",
      "the\n",
      "exercise\n",
      "of\n",
      "-PRON-\n",
      "prerogative\n",
      "have\n",
      "the\n",
      "power\n",
      "of\n",
      "proclaim\n",
      "martial\n",
      "law\n",
      "when\n",
      "-PRON-\n",
      "appear\n",
      "to\n",
      "-PRON-\n",
      "to\n",
      "be\n",
      "call\n",
      "for\n",
      "by\n",
      "the\n",
      "circumstance\n",
      "under\n",
      "which\n",
      "any\n",
      "part\n",
      "of\n",
      "the\n",
      "empire\n",
      "may\n",
      "be\n",
      "place\n",
      " \n",
      "-PRON-\n",
      "be\n",
      "impossible\n",
      "in\n",
      "the\n",
      "nature\n",
      "of\n",
      "thing\n",
      "that\n",
      "excess\n",
      "could\n",
      "be\n",
      "altogether\n",
      "avoid\n",
      " \n",
      "-PRON-\n",
      "must\n",
      "be\n",
      "permit\n",
      "to\n",
      "protest\n",
      "against\n",
      "any\n",
      "such\n",
      "inference\n",
      "which\n",
      "do\n",
      "not\n",
      "at\n",
      "all\n",
      "appear\n",
      "to\n",
      "-PRON-\n",
      "to\n",
      "be\n",
      "conclusive\n",
      " \n",
      "-PRON-\n",
      "be\n",
      "an\n",
      "arrangement\n",
      "for\n",
      "afford\n",
      "to\n",
      "the\n",
      "wellaffected\n",
      "part\n",
      "of\n",
      "the\n",
      "empire\n",
      "the\n",
      "enjoyment\n",
      "of\n",
      "-PRON-\n",
      "usual\n",
      "privilege\n",
      "and\n",
      "at\n",
      "the\n",
      "same\n",
      "time\n",
      "arm\n",
      "government\n",
      "with\n",
      "such\n",
      "power\n",
      "as\n",
      "would\n",
      "enable\n",
      "-PRON-\n",
      "to\n",
      "put\n",
      "down\n",
      "rebellion\n",
      "whether\n",
      "openly\n",
      "array\n",
      "in\n",
      "the\n",
      "field\n",
      "or\n",
      "secretly\n",
      "meditate\n",
      "the\n",
      "mean\n",
      "of\n",
      "renew\n",
      "insurrection\n",
      " \n",
      "for\n",
      "-PRON-\n",
      "own\n",
      "part\n",
      "-PRON-\n",
      "be\n",
      "quite\n",
      "at\n",
      "a\n",
      "loss\n",
      "to\n",
      "know\n",
      "what\n",
      "be\n",
      "the\n",
      "state\n",
      "of\n",
      "that\n",
      "country\n",
      "from\n",
      "any\n",
      "thing\n",
      "which\n",
      "have\n",
      "fall\n",
      "from\n",
      "-PRON-\n",
      "majesty\n",
      "minister\n",
      " \n",
      "-PRON-\n",
      "wish\n",
      "to\n",
      "explain\n",
      "to\n",
      "the\n",
      "noble\n",
      "secretary\n",
      "of\n",
      "state\n",
      "opposite\n",
      "lord\n",
      "hawkesbury\n",
      "the\n",
      "view\n",
      "which\n",
      "-PRON-\n",
      "entertain\n",
      "en\n",
      "this\n",
      "subject\n",
      "and\n",
      "the\n",
      "sentiment\n",
      "which\n",
      "-PRON-\n",
      "feel\n",
      "respect\n",
      "the\n",
      "good\n",
      "mode\n",
      "of\n",
      "proceed\n",
      "in\n",
      "the\n",
      "redress\n",
      "of\n",
      "irish\n",
      "grievance\n",
      " \n",
      "follow\n",
      "and\n",
      "take\n",
      "occasion\n",
      "to\n",
      "animadvert\n",
      "strongly\n",
      "on\n",
      "the\n",
      "situation\n",
      "in\n",
      "which\n",
      "-PRON-\n",
      "lordship\n",
      "be\n",
      "call\n",
      "on\n",
      "to\n",
      "pass\n",
      "bill\n",
      "of\n",
      "the\n",
      "nature\n",
      "which\n",
      "be\n",
      "now\n",
      "under\n",
      "-PRON-\n",
      "consideration\n",
      " \n",
      "when\n",
      "-PRON-\n",
      "say\n",
      "this\n",
      "-PRON-\n",
      "wish\n",
      "to\n",
      "shew\n",
      "-PRON-\n",
      "lordship\n",
      "how\n",
      "-PRON-\n",
      "observation\n",
      "be\n",
      "fully\n",
      "justify\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    print(word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTokens(spacy_doc_object):\n",
    "    spacy_tokens = []\n",
    "    for doc in spacy_doc_object:\n",
    "        for token in doc:\n",
    "            row = (token.text, token.lemma_, token.pos_, token.dep_, token.head.text)\n",
    "            spacy_tokens.append(row)\n",
    "    return spacy_tokens\n",
    "\n",
    "reddit_tokens = extractTokens(reddit_sample_he_she_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, nsubjpass, VERB\n",
    "\n",
    "def extractPairs(spacy_doc_object):\n",
    "    pairs = []\n",
    "    for doc in spacy_doc_object:\n",
    "        for subject in doc:\n",
    "            if subject.dep == nsubj or subject.dep == nsubjpass and subject.head.pos == VERB:\n",
    "                extracted_pairs = subject.text, subject.head.lemma_\n",
    "                concat_extracted_pairs = ' '.join(extracted_pairs)\n",
    "                pairs.append(str(concat_extracted_pairs))\n",
    "    return pairs\n",
    "\n",
    "pairs = extractPairs(reddit_sample_he_she_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_pairs = [word.lower() for word in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_he(pairs):\n",
    "    regex = re.compile('he ')\n",
    "    male = [word for word in lowercase_pairs if regex.match(word)]\n",
    "    return male\n",
    "\n",
    "male = extractMalePairs(lowercase_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['right', 'hon', 'general', 'mr', 'shall', 'majesty', 'exchequer', 'address', 'motion', 'bill',\n",
    "             'earl', 'friend', 'chancellor', 'sense', 'object', 'suppose', 'amidst', 'noble', 'lord', 'agree',\n",
    "             'speech', 'kind', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september',\n",
    "             'october', 'november', 'december', 'order', 'held', 'hold', 'assembly', '£', 'humble', 'about', 'above',\n",
    "             'across', 'address', 'after', 'afterwards', 'again', 'against', 'agree', 'all', 'almost', 'alone', 'along',\n",
    "             'already', 'also', 'although', 'always', 'am', 'amidst', 'among', 'amongst', 'amount', 'an', 'and', 'another',\n",
    "             'any', 'anyhow', 'anything', 'anyway', 'anywhere', 'april', 'are', 'around', 'as', 'at', 'back', 'be', 'was',\n",
    "             'becames', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being',\n",
    "             'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'do',\n",
    "             'could', 'did', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'either', 'else', 'elsewhere',\n",
    "             'empty', 'enough', 'even', 'ever', 'everyone', 'everything', 'everywhere', 'except', 'few', 'first', 'for',\n",
    "             'from', 'front', 'full', 'further', 'general', 'get', 'give', 'go', 'had', 'has', 'have', 'held', 'hence',\n",
    "             'here', 'hereby', 'herein', 'hold', 'however', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself',\n",
    "             'just', 'keep', 'kind', 'last', 'latter', 'least', 'less', 'made', 'make', 'many', 'me', 'meanwhile', 'might',\n",
    "             'mine', 'more', 'most', 'mostly', 'move', 'much', 'must', 'name,' 'neither', 'never', 'nevertheless', 'next',\n",
    "             'no', 'noble', 'nobody', 'none', 'nor', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'one', 'once',\n",
    "             'only', 'onto', 'or', 'other', 'others', 'otherwise', 'out', 'over', 'own', 'part', 'perhaps', 'please', 'put',\n",
    "             'quite', 'rather', 'really', 'regarding', 'same', 'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sense',\n",
    "             'several', 'shall', 'should', 'show', 'side', 'since', 'so', 'some', 'someone', 'something', 'sometime',\n",
    "             'still', 'such', 'suppose', 'take', 'than', 'that', 'the', 'then', 'there', 'thereby', 'therefore', 'these',\n",
    "             'they', 'this', 'those', 'though', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'under',\n",
    "             'unless', 'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'was', 'we', 'well', 'were', 'what',\n",
    "             'whatever', 'when', 'when', 'whereby', 'whether', 'which', 'while', 'who', 'why', 'will', 'with', 'within',\n",
    "             'without', 'would', 'yet', 'bright', 'mr.', 'hansard', 'lancashire', '[]', '£1', '000', 'russell', 'committee',\n",
    "             'reading', 'learned', 'deal', 'time', 'royal', 'gentlemen', 'gentleman', 'year', 'years', 'affairs', 'affair',\n",
    "             'academy', 'sir', 'thought', 'took', 'bring', 'brings', 'brought', 'forward', 'great', 'good', 'department',\n",
    "             'treasury', 'second', 'take', 'taken', 'privy', 'member', 'robert', 'large', 'session', 'secretary', 'notice',\n",
    "             'moment', 'think', 'believe', 'hope', 'ask', 'hear', 'beg', 'support', 'state', 'find', 'admit', 'wish',\n",
    "             'refer', 'reply', 'know', 'feel', 'propose', 'understand', 'let', 'allow', 'like', 'receive', 'consider',\n",
    "             'begin', 'tell', 'like', 'send', 'ought', 'come', 'intend', 'add', 'want', 'stand', 'suggest', 'remind',\n",
    "             'use', 'mean', 'suggest', 's']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(pairs):\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        tokens = pair.split(\" \")\n",
    "        tokens_filtered = [word for word in tokens if not word in stopwords]\n",
    "        joined_tokens = \" \".join(tokens_filtered)\n",
    "        if len(tokens_filtered) == 2: # if string is length of two\n",
    "            keep_pairs.append(str(joined_tokens)) # keep the string\n",
    "    return keep_pairs\n",
    "\n",
    "male_no_stopwords = removeStopWords(male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWords(clean_data):\n",
    "    ngrams_dict = {}\n",
    "    for ngram in clean_data:\n",
    "        if ngram in ngrams_dict:\n",
    "            ngrams_dict[ngram] += 1\n",
    "        else:\n",
    "            ngrams_dict[ngram] = 1\n",
    "    return ngrams_dict\n",
    "            \n",
    "male_dictionary = countWords(male_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kv in list(male_dictionary)[:30]:\n",
    "    print(kv,  male_dictionary[kv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get pandas code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
